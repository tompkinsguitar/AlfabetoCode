{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "# from Continuo.kmeans_inversion import k_means_data\n",
    "import numpy as np\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import Continuo.ContinuoConverter as CC\n",
    "from alfabeto_data.pickled_data import *\n",
    "from alfabeto_sources import all_sources\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import pickle, threading, time, sys\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = corpus.getComposer('palestrina')[0:400]\n",
    "all_parsed = []\n",
    "all_keys = []\n",
    "chordified = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crd_data(unparsed):  \n",
    "    def parser(song):\n",
    "        x = converter.parse(song)\n",
    "        if type(x) is stream.Score:\n",
    "            crd = x.chordify().flat.getElementsByClass(chord.Chord)\n",
    "            key_sig = x.flat.getElementsByClass(key.KeySignature)[0].sharps\n",
    "            bass_note = crd[-1].bass().pitchClass\n",
    "            chordified.append(crd)\n",
    "            all_keys.append((key_sig, bass_note))\n",
    "        elif type(x) is stream.Opus:\n",
    "            for y in x:\n",
    "                crd = y.chordify().flat.getElementsByClass(chord.Chord)\n",
    "                key_sig = y.flat.getElementsByClass(key.KeySignature)[0].sharps\n",
    "                bass_note = crd[-1].bass().pitchClass\n",
    "                chordified.append(crd)\n",
    "                all_keys.append((key_sig, bass_note))\n",
    "    start = time.time()\n",
    "    for x in unparsed:\n",
    "        st = time.time()\n",
    "        parser(x)\n",
    "        fin = time.time()\n",
    "        print('finished', len(chordified), 'of', len(unparsed), 'in', fin - st, 'seconds')\n",
    "    print('entire process took:', time.time() - start, 'seconds')\n",
    "\n",
    "    palestrina_chords_pickle_out = open(\"pickles/temp/palestrina_chords_temp_1.pickle\", \"wb\")\n",
    "    pickle.dump((chordified, all_keys), palestrina_chords_pickle_out)\n",
    "    palestrina_chords_pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crd_data(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(all_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpus_continuo(corpus_unparsed):\n",
    "    st = time.time()\n",
    "    all_continuo_data = []\n",
    "    all_corpus_labels = []\n",
    "    for x in corpus_unparsed:\n",
    "        y = corpus.parse(x)\n",
    "        both_parts = []\n",
    "        harmony_notes = []\n",
    "        bass_notes = []\n",
    "        \n",
    "        if type(y) is stream.Score:\n",
    "            z = y.chordify().flat.getElementsByClass(chord.Chord)\n",
    "            bass_note = z[-1].bass().pitchClass\n",
    "            theKey = y.flat.getElementsByClass(key.KeySignature)\n",
    "            all_corpus_labels.append((theKey[0].sharps, bass_note))\n",
    "            for a in z:\n",
    "                harmony = []\n",
    "                root_note = (a.bass().pitchClass - bass_note)%12\n",
    "                bass_notes.append(root_note)\n",
    "                for b in a:\n",
    "                    root_note = (a.bass().pitchClass - bass_note)%12\n",
    "                    harmony.append((b.pitchClass - bass_note - root_note) %12)\n",
    "                harmony_notes.append(sorted(set(harmony)))\n",
    "\n",
    "        \n",
    "        elif type(y) is stream.Opus:\n",
    "            for yy in y:\n",
    "                z = yy.chordify().flat.getElementsByClass(chord.Chord)\n",
    "                bass_note = z[-1].bass().pitchClass\n",
    "                theKey = yy.flat.getElementsByClass(key.KeySignature)\n",
    "                all_corpus_labels.append((theKey[0].sharps, bass_note))\n",
    "                \n",
    "                for a in z:\n",
    "                    harmony = []\n",
    "                    bass_notes.append((a.bass().pitchClass - bass_note)%12)\n",
    "                    for b in a:\n",
    "                        harmony.append((b.pitchClass - bass_note - (a[-1].pitchClass - bass_note)%12))\n",
    "                    harmony_notes.append(sorted(set(harmony)))\n",
    "            for a, b in zip(bass_notes, harmony_notes):\n",
    "                both_parts.append((a, b))\n",
    "\n",
    "        for a, b in zip(bass_notes, harmony_notes):\n",
    "            both_parts.append((a, b))\n",
    "\n",
    "        all_continuo_data.append(both_parts)\n",
    "    print(time.time() - st, 'seconds')\n",
    "    return all_continuo_data, all_corpus_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = corpus_continuo(corpus.getComposer('palestrina')[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bach_data = corpus_continuo(A)\n",
    "bach_harm_pickle_out = open('pickles/bach_harm.pickle', 'wb')\n",
    "pickle.dump(bach_data, bach_harm_pickle_out)\n",
    "bach_harm_pickle_out.close()\n",
    "\n",
    "bach_harm_pickle_in = open('pickles/bach_harm.pickle', 'rb')\n",
    "bach_harm_data = pickle.load(bach_harm_pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def figure_sorter(list_tuple_data):\n",
    "    final_data = []\n",
    "    figures = [[0, 4, 7], [0, 3, 7], [0, 3, 8], [0, 4, 9], [0, 5, 9], [0, 5, 8]]\n",
    "    for song in list_tuple_data:\n",
    "        chord_number_total = len(song)\n",
    "        temp_data = []\n",
    "#         temp_data_other = []\n",
    "        for figure in figures:\n",
    "            figure_list = []\n",
    "            for number in range(12):\n",
    "                chord_number = 0\n",
    "                for harmony in song:\n",
    "                    if figure == harmony[1] and number == harmony[0]:\n",
    "                        chord_number += 1\n",
    "\n",
    "                if chord_number >0:\n",
    "                    temp_data.append(chord_number/chord_number_total * 100)\n",
    "                else:\n",
    "                    temp_data.append(0)\n",
    "        other_figures = []\n",
    "        for number in range(12):\n",
    "            number_other = 0\n",
    "            for harmony in song:\n",
    "                if harmony[0] == number and harmony[1] not in figures:\n",
    "                    number_other += 1\n",
    "            if number_other > 0:\n",
    "                other_figures.append(number_other/chord_number_total*100)\n",
    "            else:\n",
    "                other_figures.append(0)\n",
    "                   \n",
    "        \n",
    "        for x in other_figures:\n",
    "            temp_data.append(x)\n",
    "        final_data.append(temp_data)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inversion_data(corpus):\n",
    "    continuo_corpus = []\n",
    "    all_data = []\n",
    "    for x in corpus:\n",
    "        continuo_corpus.append(CC.book_converter_pc(x, 'all'))\n",
    "    for w in continuo_corpus:\n",
    "\n",
    "        for thing in w:\n",
    "            frequency_dict = {}\n",
    "            total_numbers = 0\n",
    "            freq_list = []\n",
    "            for number in range(12):\n",
    "                for inversion_list in converters.triad_inversions_pc:\n",
    "                    for inversion in inversion_list:\n",
    "                        frequency_dict[str((number, inversion))] = 0\n",
    "                        for x in thing:\n",
    "                            if x[0] == number and x[1] == inversion:\n",
    "                                frequency_dict[str((x[0], inversion))] += 1\n",
    "                                total_numbers += 1\n",
    "        # print(frequency_dict, total_numbers)\n",
    "            for number in range(12):\n",
    "                for inversion_list in converters.triad_inversions_pc:\n",
    "                    for inversion in inversion_list:\n",
    "                        for x, y in frequency_dict.items():\n",
    "                            if x == str((number, inversion)):\n",
    "                                # freq_list.append(y/total_numbers * 100)\n",
    "                                freq_list.append(y/total_numbers * 100)\n",
    "            # print(frequency_dict, total_numbers)\n",
    "            all_data.append(freq_list)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bach_labeler(list_of_lists):\n",
    "    bach_labels = []\n",
    "    for x in list_of_lists[1]:\n",
    "        bach_labels.append('$'+str(x[0])+':'+str(x[1])+'$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bach_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_means_data(list_of_lists, cluster_number, label_markers):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    digits = np.array(list_of_lists)\n",
    "    # data = scale(digits)\n",
    "    data = digits\n",
    "    label_dict = {}\n",
    "    label_numbers = []\n",
    "    n = 0\n",
    "    label_dict[label_markers[0]] = 0\n",
    "    for j in range(1, len(label_markers)):\n",
    "        if label_markers[j] not in label_dict:\n",
    "            label_dict[label_markers[j]] = n+1\n",
    "            n += 1\n",
    "\n",
    "    for i in label_markers:\n",
    "        label_numbers.append(label_dict[i])\n",
    "    n_samples, n_features = data.shape\n",
    "    n_digits = cluster_number\n",
    "    labels = np.array(label_numbers)\n",
    "    # print('label numbers', label_numbers)\n",
    "    # sample_size = 300\n",
    "\n",
    "    sample_size = len(data)\n",
    "    # print(sample_size, 'Sample Size')\n",
    "\n",
    "\n",
    "    print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "          % (n_digits, n_samples, n_features))\n",
    "\n",
    "    print(79 * '_')\n",
    "    print('% 9s' % 'init'\n",
    "                   '        time     inertia  homo     compl   v-meas  ARI  AMI   silhouette')\n",
    "\n",
    "    def bench_k_means(estimator, name, data):\n",
    "        t0 = time()\n",
    "        estimator.fit(data)\n",
    "        print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n",
    "              % (name, (time() - t0), estimator.inertia_,\n",
    "                 metrics.homogeneity_score(labels, estimator.labels_),\n",
    "                 metrics.completeness_score(labels, estimator.labels_),\n",
    "                 metrics.v_measure_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_mutual_info_score(labels, estimator.labels_),\n",
    "                 metrics.silhouette_score(data, estimator.labels_,\n",
    "                                          metric='euclidean',\n",
    "                                          sample_size=sample_size)))\n",
    "\n",
    "\n",
    "    bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "                  name=\"k-means++\", data=data)\n",
    "\n",
    "    bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "                  name=\"random\", data=data)\n",
    "\n",
    "    # in this case the seeding of the centers is deterministic, hence we run the\n",
    "    # kmeans algorithm only once with n_init=1\n",
    "    pca = PCA(n_components=n_digits).fit(data)\n",
    "    bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "                  name=\"PCA-based\",\n",
    "                  data=data)\n",
    "    print(79 * '_')\n",
    "\n",
    "    ###############################################################################\n",
    "    # Visualize the results on PCA-reduced data\n",
    "\n",
    "    reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "    kmeans.fit(reduced_data)\n",
    "\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .02  # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')\n",
    "\n",
    "    for x, y, z in zip(reduced_data[:, 0], reduced_data[:, 1], label_markers):\n",
    "        if z == '$Temperley MAJOR$':\n",
    "            plt.plot(x, y, 'k.', marker='$M$', color='white', markeredgecolor='none', markersize=40)\n",
    "        elif z == '$Temperley MINOR$':\n",
    "            plt.plot(x, y, 'k.', marker='$m$', color='white', markeredgecolor='none', markersize=40)\n",
    "        elif z[1] == '*':\n",
    "            plt.plot(x, y, 'k.', marker=z, color='white', markeredgecolor='none', markersize=40)\n",
    "        else:\n",
    "            plt.plot(x, y, 'k.', marker=z, alpha=.3, markersize=len(z)*4)\n",
    "#     plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.')\n",
    "        # plt.plot(x, y, 'k.')\n",
    "\n",
    "    # for x in alf_labels:\n",
    "    #         plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', marker = x, markersize=5)\n",
    "    #     print('reduced data:', reduced_data)\n",
    "    # Plot the centroids as a white X\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=169, linewidths=3,\n",
    "                color='w', zorder=10)\n",
    "#     plt.title('K-means clustering of the corpus (PCA-reduced data)\\n'\n",
    "#               'Centroids are marked with white cross')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    # plt.show()\n",
    "    k_data = KMeans(init='k-means++', n_clusters=n_digits, n_init=10).fit(data)\n",
    "    # return metrics.silhouette_score(data, k_data.labels_, metric='euclidean', sample_size=sample_size)\n",
    "    return reduced_data, metrics.silhouette_score(data, k_data.labels_, metric='euclidean', sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from alfabeto_data.pickled_data import *\n",
    "\n",
    "def label_maker_alf(corpus_list):\n",
    "\n",
    "#     final_labels = []\n",
    "    alf_labels = []\n",
    "#     for x in corpus_list:\n",
    "#         for z in x.values():\n",
    "#             print(z['data']['key'])\n",
    "#             a = z['data']['key']\n",
    "#             b = z['data']['final']\n",
    "#             c = (a, b)\n",
    "#             final_labels.append(c)\n",
    "    \n",
    "#     for x in final_labels:\n",
    "    for x in corpus_list: #do this if the tuples are in list of list\n",
    "\n",
    "        if x == ('f',2):\n",
    "            alf_labels.append('$♭:D$')\n",
    "        elif x == ('n',4):\n",
    "            alf_labels.append('$♮:E$')\n",
    "        elif x == ('n',9):\n",
    "            alf_labels.append('$♮:A$')\n",
    "        elif x == ('f',7):\n",
    "            alf_labels.append('$♭:G$')\n",
    "        elif x == ('n',2):\n",
    "            alf_labels.append('$♮:D$')\n",
    "        elif x == ('f',9):\n",
    "            alf_labels.append('$♭:A$')\n",
    "        elif x == ('f',0):\n",
    "            alf_labels.append('$♭:C$')\n",
    "        elif x == ('f',10):\n",
    "            alf_labels.append('$♭:B♭$')\n",
    "        elif x == ('n',5):\n",
    "            alf_labels.append('$♮:F$')\n",
    "        elif x == ('n',0):\n",
    "            alf_labels.append('$♮:C$')\n",
    "        elif x == ('f',5):\n",
    "            alf_labels.append('$♭:F$')\n",
    "        elif x == ('n',7):\n",
    "            alf_labels.append('$♮:G$')\n",
    "        else:\n",
    "            alf_labels.append('o')\n",
    "    return alf_labels\n",
    "\n",
    "%matplotlib inline\n",
    "for x in range(2, 7):\n",
    "    k_means_data(figure_sorter(inversion_data(all_sources.GetAll.all_alf)), x, label_maker_alf(alfabeto_notes_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bach_harm_pickle_out = open('pickles/bach_harm.pickle', 'wb')\n",
    "# pickle.dump((figure_sorter(bach_harm_data[0]), bach_harm_data[1]), bach_harm_pickle_out)\n",
    "# bach_harm_pickle_out.close()\n",
    "\n",
    "# bach_harm_pickle_in = open('pickles/bach_harm.pickle', 'rb')\n",
    "# bach_harm_data = pickle.load(bach_harm_pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bach_harm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# k_bach = k_means_data(figure_sorter(bach_data[0]), 2, bach_labels)\n",
    "\n",
    "for x in range(2, 7):\n",
    "    print(k_means_data(figure_sorter(bach_data[0]), x, bach_labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_ready = [sum(elem)/len(elem) for elem in zip(*figure_sorter(bach_data[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(figure_sorter(bach_data[0])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modal_labels = []\n",
    "for x, y in zip(bach_data[0], bach_data[1]):\n",
    "    if y == (-1, 5):\n",
    "        modal_labels.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_ready = [sum(elem)/len(elem) for elem in zip(*figure_sorter(modal_labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_readyR = []\n",
    "for a in range(7):\n",
    "    x = a*12\n",
    "    csv_readyR.append(csv_ready[x:x+12])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_readyR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/home/daniel/Desktop/bach_continuo.csv', 'w', newline='') as fp:\n",
    "    a = csv.writer(fp, delimiter=',')\n",
    "    final_data = csv_readyR\n",
    "    a.writerows(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(u'\\u266f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(u'\\u1D130'.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = u'\\u1D130'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unicodedata.lookup('FLAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
